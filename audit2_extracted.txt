Analyse von "sauber_main" (Sheratan Clean Build)
Vorgehen und Testdurchführung
Code-Exploration
Der hochgeladene Zip‐Ordner wurde entpackt und der Inhalt untersucht.
Die Verzeichnisstruktur entspricht der „Sheratan Clean Build“‑Version.
Das Projekt besteht aus mehreren Subsystemen: mesh (Broker/Hosts/Runtime), core (FastAPI‑API), worker (LCP‑Arbeitsloop), external (LLM‑Bridge, Gatekeeper, Auditor, Final Decision) und dashboard (React‑UI).
In der öffentlich zugänglichen README des GitHub‑Repos wird dieses Setup beschrieben[1].
Ausführen ausgewählter Tests
Aufgrund der beschränkten Umgebung wurden nur Python‑Tests ausgeführt. Hierfür waren bereits viele Abhängigkeiten installiert (fastapi, pydantic usw.). Tests, die python‑dotenv oder Node.js benötigten, konnten ohne zusätzliche Pakete nicht ausgeführt werden.
Atomic‑IO‑Test: tests/test_atomic_save_stress.py startete vier parallele Prozesse, die dieselbe JSON‑Datei 100× schrieben. Das Ergebnis zeigte keine Datenkorruption, was auf eine robuste Umsetzung des Dateilocks und atomischen Schreibens hindeutet.
Lost‑Update‑Test: tests/test_locking_lost_updates.py demonstrierte, dass der Lock‐Mechanismus in core/utils/atomic_io.py verlorene Updates verhindert. Ohne Lock wurden nur 63 von 200 erwarteten Inkrementen persistiert; mit Lock stimmte die Zahl exakt.
Ledger‑Throughput‑Test: tests/test_throughput.py führte 100 Einzeltransaktionen und 100 Transaktionen in 10er‑Batches durch. Das Batching erhöhte die Durchsatzrate von ca. 152 auf 278 Transaktionen/s (≈1,8×).
Multi‑Node‑Sync: tests/test_multinode_sync.py startete einen Ledger‑Writer‑Dienst, erzeugte zehn Gutschriften und synchronisierte anschließend eine Replik. Die Bilanz des Arbeiters stimmte überein, was auf ein funktionierendes Replikations‑ und Journal‑System hinweist.
Analyse der Kernmodule
Dateibasierte Persistenz und Locks: In core/utils/atomic_io.py werden JSON‑Dateien mit temporären Dateien geschrieben und anschließend per os.replace atomar überschrieben. Ein Begleit‐Lock (json_lock) stellt sicher, dass konkurrierende Prozesse nicht gleichzeitig schreiben. Dieses Vorgehen reduziert Datenkorruption und verhindert verlorene Updates, verursacht aber jeweils Dateisystem‑I/O und fsync‑Aufrufe, die bei hoher Last ein Bottleneck darstellen können.
State Machine (core/state_machine.py): Die Systemzustände (OPERATIONAL, DEGRADED, REFLECTIVE, RECOVERY, PAUSED) werden in einer JSON‑Datei gespeichert; Übergänge werden in eine JSONL‑Datei geschrieben. Die Implementierung ist einfach und robust, aber _append_event öffnet die Logdatei bei jedem Übergang erneut und schreibt synchron. Bei häufiger Zustandsänderung könnte das zu I/O‑Overhead führen.
Worker‑Loop: Der Worker überwacht einen Ordner (data/webrelay_out) und verarbeitet dort abgelegte .job.json‑Dateien. Am Ende jeder Schleife wartet er eine Sekunde (time.sleep(1.0)). Dieses Polling ist einfach, führt aber bei vielen Jobs zu Latenzen und unnötigen Dateisystem‑Scans. Außerdem werden Ergebnisse synchron per HTTP an den Core‑Service gemeldet (requests.post), wodurch der Worker bei Netzwerkproblemen blockiert.
Ledger‑Service (mesh/registry/ledger_service.py): Das Ledger verwaltet Kontostände mit einem In‑Memory‑State und persistiert nach jeder Operation die komplette JSON‑Datei (save_state). Ein Lock (threading.Lock) synchronisiert den Zugriff innerhalb des Prozesses, während json_lock die Domain‑Datei über Prozesse hinweg schützt. Mit zunehmender Anzahl von Transaktionen führt das ständige Laden und Speichern zu wachsendem I/O‑Overhead und kann zum Engpass werden. Der Test test_throughput zeigt, dass Batching den Durchsatz verbessert, dennoch bleibt der Ansatz dateibasiert.
Health‑Prober & Registry (nicht getestet): Das Modul mesh/registry/health_prober.py führt regelmäßige HTTP‑Pings zu Host‑Endpoints aus und aktualisiert den Status in einer JSON‑Registry. Ohne asynchrones Framework sind diese Pings sequentiell, was die Erkennungszeit bei vielen Hosts erhöht.
Gefundene Bottlenecks und Verbesserungspotentiale
Polling‑Schleifen – sowohl der Worker‑Loop als auch andere Komponenten (z. B. core/chain_runner.py) verwenden feste time.sleep‑Pausen (eine Sekunde oder länger). Diese Perioden verursachen Latenzen und limitieren den Durchsatz. Ein eventgesteuerter Ansatz (Dateisystem‑Watcher wie watchdog oder Inotify, message queue) könnte Jobs nahezu in Echtzeit verarbeiten.
Dateibasierte Queues und Logs – Die Eingabe‑/Ausgabe‑Ordner (mesh/runtime/inbox, queue, outbox) werden mittels Dateisystem verwaltet. Bei einer großen Zahl von Dateien werden Verzeichnis‑Scans teuer. Ein leichtgewichtiges Datenbank‑Backend (z. B. SQLite oder LiteFS) kann das Queue‑Management effizienter gestalten, insbesondere wenn Idempotenz, Abhängigkeiten und Prioritäten umgesetzt werden sollen (siehe Zukunftsplan im Projekt, u. a. Prioritäts‑Queues und SQLite).
Synchrones HTTP und I/O – Der Worker sendet für jedes Ergebnis einen HTTP‑Request und wartet synchron auf die Antwort. Bei instabilen Netzen blockiert dies den gesamten Loop. Die Verwendung asynchroner HTTP‑Clients (z. B. httpx.AsyncClient) oder eine Message‑Queue (RabbitMQ, Redis Streams) entkoppelt Job‑Abarbeitung und Ergebnis‑Übermittlung.
Globales Dateilocking – json_lock legt zu jedem JSON eine .lock‑Datei an. Dieser Mechanismus funktioniert pro Datei, erlaubt aber keine parallelen Operationen an unterschiedlichen Teilen des States und kann bei vielen konkurrierenden Prozessen zu „lock contention“ führen. Feingranularere Locks (z. B. nur pro Konto) oder eine transaktionale DB würden die Parallelität erhöhen.
Kumulative JSON‑Snapshots – Der Ledger speichert den kompletten Kontostand bei jedem Write. Der Zustand kann mit wachsender Anzahl von Accounts sehr groß werden. Stattdessen könnte man Delta‑Events plus periodische Snapshots mit Checkpoint‑Pruning verwenden oder ein eingebettetes KV‑Store (z. B. SQLite, SQLite‑R/W‑Wal) nutzen.
Fehlende asynchrone Architektur – Viele Komponenten (API, Worker, Health‑Prober) nutzen synchrone Bibliotheken. Für ein verteiltes, autonomes Mesh könnte ein stärker asynchrones Fundament (AsyncIO, FastAPI mit async‑Endpoints, aioredis) Skalierbarkeit und Reaktionszeit erhöhen.
Logging‑Overhead – Die State‑Machine und andere Module öffnen Log‑Dateien bei jedem Schreiben neu. Durch Verwendung eines rotierenden Log‑Handlers oder persistenter File‑Handles kann die Anzahl der Datei‑Öffnungen reduziert werden.
Konfigurationsabhängigkeiten – Einige Tests scheiterten, weil python‑dotenv nicht vorhanden war. Das System sollte so gestaltet werden, dass fehlende optionale Abhängigkeiten die Grundfunktionen nicht blockieren (Fallbacks oder In‑Code‑Defaults).
Empfohlene nächste Schritte für die Weiterentwicklung
Einführung eines echten Message‑/Job‑Queue‑Backends
Evaluieren Sie anstelle der dateibasierten Inbox/Queue/Outbox eine leichtgewichtige persistent‑Queue wie SQLite (wie im Refactoring‑Plan erwähnt) oder eine dedizierte Message‑Queue (Redis, RabbitMQ). Dies reduziert I/O‑Kosten und erlaubt atomare Abarbeitung, Priorisierung und Rückmeldungen.
Asynchrone Verarbeitung und Event‑Driven Design
Reduzieren Sie Polling und nutzen Sie Datei‑/Queue‑Watcher oder asynchrone Event‑Schleifen. Für den Worker können asyncio‑Tasks parallel Ergebnisse schreiben und HTTP‑Aufrufe nicht blockieren. Die Health‑Prober können mit aiohttp parallel pingen.
Optimierung der Ledger‑Persistence
Implementieren Sie Deltas und Snapshots: statt jedes Mal den gesamten Kontostand zu speichern, nur Änderungen schreiben und in regelmässigen Abständen einen Snapshot der Bilanz anlegen.
Nutzen Sie einen eingebetteten Datenbank‑Layer (SQLite) mit Transaktionen, um parallele Zugriffe zu ermöglichen. Dadurch entfallen .lock‑Dateien und fsync‑Aufrufe.
Durchsatzsteigerung durch Batching und Pipeline‑Verarbeitung
Der Test test_throughput zeigte bereits eine 1,8×‑Steigerung durch Batching. Allgemein sollten API‑Calls und Ledger‑Operationen gebündelt werden, um die Anzahl der I/O‑Operationen zu reduzieren.
Robustere Konfiguration und Abhängigkeitsmanagement
.env‑Variablen sollten optional sein; wenn dotenv nicht verfügbar ist, sollten vernünftige Defaults verwendet werden.
Ein zentraler pyproject.toml oder requirements.txt im Projektroot könnte die Installation erleichtern.
Implementierung der angekündigten Production‑Features
Laut README und Refactoring‑Plan gehören Idempotenz, Retry‑Logik, Timeouts, Prioritäts‑Queues, SQLite‑Storage, Host‑Health‑Checks und Rate‑Limiting zu den vorgesehenen Features[2]. Diese sollten priorisiert werden, um Stabilität und Skalierbarkeit zu erhöhen.
Testabdeckung erweitern
Die vorhandenen Tests decken Atomic‑Operationen und Ledger‑Logik gut ab, aber Tests für HTTP‑APIs (FastAPI), Health‑Prober und Worker‑Loop fehlen wegen fehlender Abhängigkeiten.
Integrationstests, die mehrere Komponenten gleichzeitig starten (Core API, Broker, Hosts), könnten End‑to‑End‑Szenarien validieren.
Dokumentation kontinuierlich aktualisieren
Die README enthält bereits eine gute Übersicht über Ports und Komponenten[3]. Ergänzen Sie die Dokumentation um Informationen zu Performance‑Grenzen, Konfigurationsoptionen und Migrationspfaden von der dateibasierten Architektur zu Datenbank‑Backends.
Fazit
Die Sheratan‑Plattform in ihrer „sauberen“ Version ist modular aufgebaut und arbeitet stabil im Kleinen. Die Tests zeigten robuste Mechanismen zur Vermeidung von Datenkorruption sowie eine erkennbare Performance‑Steigerung durch Batching. Dennoch ist die Architektur stark dateibasiert und synchron, was bei höherem Lastprofil zu Engpässen führen wird. Die nächste Entwicklungsphase sollte sich auf die Einführung einer persistenten Queue/Datenbank, asynchroner Verarbeitung und der im Refactoring‑Plan genannten Production‑Features konzentrieren, um die Effizienz und Skalierbarkeit zu verbessern, ohne die klaren Grenzen zwischen Mesh‑Intern und Mesh‑Extern sowie die Philosophie der Nachvollziehbarkeit zu verlieren.

[1] [2] [3] GitHub - J3r3C0/sauber_main
https://github.com/J3r3C0/sauber_main